{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e803765",
   "metadata": {},
   "source": [
    "# Add needed parameters to site files to run GMPE for Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c1d95",
   "metadata": {},
   "source": [
    "### Notes\n",
    "This notebook should be run after running 3_oq_vs30_uniform_grid.ipynb where the site and station_site files are creates\n",
    "\n",
    "It brings most of the scripts and code from https://gitlab.seismo.ethz.ch/efehr/esrm20_sitemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65150492",
   "metadata": {},
   "source": [
    "##  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0962ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chago/openquake/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import utils\n",
    "from plots import gdf_epicentre, stations_plot\n",
    "from openquake.commands.prepare_site_model import main as oq_site_model\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import h5py\n",
    "import argparse\n",
    "from scipy import stats\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from pyproj import CRS, Transformer\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706a100",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabe9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events list from ECD\n",
    "event = 'DRAFT_20110511_M5.1_Lorca'\n",
    "\n",
    "# Add any reference rupture\n",
    "rupture = 'earthquake_rupture_model_USGS.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45075385-a5d1-4fa0-8ef1-ff7c3bcb2fad",
   "metadata": {},
   "source": [
    "## Create region bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371819a-ffb2-4776-b1dc-4d65bbcfda7e",
   "metadata": {},
   "source": [
    "### Get file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34426665-cd43-42f1-9ba4-78912d0595ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Spain/DRAFT_20110511_M5.1_Lorca/OpenQuake_gmfs\n"
     ]
    }
   ],
   "source": [
    "# Select files\n",
    "# Select rupture file and get folders\n",
    "file_path = glob.glob(os.path.join('..', '*', event, '*', rupture))[0]\n",
    "folder = file_path[:file_path.find('Rupture')]\n",
    "\n",
    "out_folder = os.path.join(folder, 'OpenQuake_gmfs')\n",
    "print(out_folder)\n",
    "\n",
    "# Read files\n",
    "file_stations=os.path.join(out_folder,'site_model_stations.csv')\n",
    "file_site_model=os.path.join(out_folder,'site_model.csv')\n",
    "\n",
    "aux_file=os.path.join(out_folder,'aux_file.csv')\n",
    "\n",
    "output_file_stations=os.path.join(out_folder,'site_model_stations_europe.csv')\n",
    "output_file_site_model=os.path.join(out_folder,'site_model_europe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d02491f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../esrm20_sitemodel/exposure2site/site_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data files\n",
    "DATA_PATH = os.path.join('..','esrm20_sitemodel','exposure2site','site_data')\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51f9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data files\n",
    "DATA_PATH = os.path.join('..','esrm20_sitemodel','exposure2site','site_data')\n",
    "\n",
    "# Geology shapefiles\n",
    "GEOLOGY_FILE = os.path.join(DATA_PATH, \"GEOL_V8_ERA2.shp\")\n",
    "# File containing 30 arc second grid of slope, geology, elevation and\n",
    "# built density\n",
    "# BBOX = [-25.0, 34.0, 45.0, 72.0]\n",
    "SITE_HDF5_FILE = os.path.join(DATA_PATH, \"europe_gebco_topography.hdf5\")\n",
    "# File containing Vs30s at 30 arcseconds using the USGS Wald & Allen method\n",
    "# BBOX = [-30;, 30., 65., 75.]\n",
    "VS30_HDF5_FILE = os.path.join(DATA_PATH, \"europe_2020_vs30.hdf5\")\n",
    "# Pre-computed grid of volcanic front distances at 5 arc-minutes\n",
    "# BBOX = [-25.0, 34.0, 45.0, 72.0]\n",
    "VF_DIST_FILE = os.path.join(DATA_PATH, \"volcanic_front_distances_5min.hdf5\")\n",
    "# Europe slice of 250 m x 250 m GHS built density dataset (in Mollewiede proj.)\n",
    "BUILT_ENV_FILE = os.path.join(DATA_PATH, \"ghs_global_250m_Europe.hdf5\")\n",
    "# ESHM20 Attenuation Region File\n",
    "REGION_FILE = os.path.join(DATA_PATH, \"ATTENUATION_REGIONS.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d39989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiteManager(object):\n",
    "    \"\"\"\n",
    "    General object for controlling the flow of the site model rendering.\n",
    "\n",
    "    For handling the output site model it is preferble to use a pandas\n",
    "    dataframe, unless the density is so large that memory usage makes this\n",
    "    impossible\n",
    "\n",
    "    Site model dataframe requires the following elements:\n",
    "\n",
    "    \"lons\": longitudes (decimal degrees)\n",
    "    \"lats\": latitudes (decimal degrees)\n",
    "    \"slope\": slope (m/s)\n",
    "    \"geology\": geological era\n",
    "    \"vs30\": Vs30 inferred from topography unless set otherwise\n",
    "    \"xvf\": Distance from the volcanic front (km)\n",
    "    \"\"\"\n",
    "    REQUIRED_HEADERS = [\"lon\", \"lat\", \"slope\", \"geology\", \"vs30\",\n",
    "                        \"vs30measured\", \"xvf\"]\n",
    "    STRING_ATTRIBS = [\"geology\", ]\n",
    "\n",
    "    def __init__(self, site_model, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        assert isinstance(site_model, gpd.GeoDataFrame)\n",
    "        # Store sites as a geopandas Geodataframe\n",
    "        self.site_model = site_model\n",
    "        site_model_crs = kwargs.get(\"site_model_crs\", \"epsg:4326\")\n",
    "        if not self.site_model.crs:\n",
    "            self.site_model.crs = {\"init\": site_model_crs}\n",
    "        self.bbox = self.site_model.total_bounds\n",
    "        self.default_conditions = {\n",
    "            \"vs30\": kwargs.get(\"default_vs30\", 800.0),\n",
    "            \"vs30measured\": kwargs.get(\"vs30measured\", 0),\n",
    "            \"z1pt0\": kwargs.get(\"default_z1pt0\", 31.7),\n",
    "            \"z2pt5\": kwargs.get(\"default_z2pt5\", 0.57),\n",
    "            \"xvf\": kwargs.get(\"xvf\", 150.),\n",
    "            \"region\": kwargs.get(\"region\", 0)}\n",
    "        # Get the ESHM regions file and re-project to xy\n",
    "        self.eshm20_regions = gpd.GeoDataFrame.from_file(REGION_FILE)\n",
    "        self.eshm20_regions.crs = {\"init\": \"epsg:4326\"}\n",
    "        self.eshm20_regions_xy = self.eshm20_regions.to_crs(\n",
    "            {\"init\": EUROPE_EQUAL_AREA})\n",
    "        # Assign ESHM20 regions\n",
    "        self.get_eshm20_region()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.site_model.shape[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.site_model)\n",
    "\n",
    "    def __iadd__(self, model):\n",
    "        # Adds a second site model concatenating the two dataframes\n",
    "        self.site_model = pd.concat([self.site_model, model.site_model])\n",
    "        # Drop duplicates\n",
    "        self.site_model.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.site_model[key].values\n",
    "\n",
    "    def __add__(self, model):\n",
    "        output_model = pd.concat([self.site_model, model.site_model])\n",
    "        output_model.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        return self(output_model)\n",
    "\n",
    "    def plot(self, key, **kwargs):\n",
    "        # Plot the site model using thr GeoDataFrame.plot() method\n",
    "        self.site_model.plot(key, **kwargs)\n",
    "\n",
    "    def get_eshm20_region(self):\n",
    "        \"\"\"\n",
    "        Determines the ESHM20 attenuation region for the respective sites and\n",
    "        adds to the site model\n",
    "        \"\"\"\n",
    "        print(\"Assigning the sites to the ESHM20 regions\")\n",
    "        # Retrieve lightweight dataframe with just the geometry and re-project\n",
    "        # the cartesian reference frame\n",
    "        sm_lite = self.site_model[[\"lat\", \"lon\", \"geometry\"]].to_crs(\n",
    "            {\"init\": EUROPE_EQUAL_AREA})\n",
    "        # Get the region assignments\n",
    "        assignments = gpd.sjoin(sm_lite, self.eshm20_regions_xy, how=\"left\")\n",
    "        if assignments.shape[0] > self.site_model.shape[0]:\n",
    "            # This can happen if the region polygons are overlapping (which\n",
    "            # they shouldn't be in this case)\n",
    "            assignments.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        # Set any nans to region 0\n",
    "        eshm20_region = assignments[\"REGION\"].values\n",
    "        eshm20_region[np.isnan(eshm20_region)] = 0.0\n",
    "        self.site_model[\"region\"] = eshm20_region.astype(int)\n",
    "\n",
    "    @classmethod\n",
    "    def site_model_from_bbox(cls, bbox, spacing, weighting, **kwargs):\n",
    "        \"\"\"\n",
    "        Defines a site model from a bounding box, downsampled at the given\n",
    "        spacing\n",
    "        \"\"\"\n",
    "        if (spacing % 30):\n",
    "            raise ValueError(\"Spacing must be a multiple of 30\")\n",
    "        num = int(spacing) // 30\n",
    "        assert bbox[2] > bbox[0]\n",
    "        assert bbox[3] > bbox[1]\n",
    "        averaging = kwargs.get(\"averaging\", \"MEAN\")\n",
    "        geol_weighting = kwargs.get(\"geol_weighting\", \"MODE\")\n",
    "        default_xvf = kwargs.get(\"default_xvf\", 150.0)\n",
    "        smooth_n = kwargs.get(\"smooth_n\", 0)\n",
    "        onshore_only = kwargs.get(\"onshore_only\", True)\n",
    "        as_dataframe = kwargs.get(\"as_dataframe\", True)\n",
    "        return cls(\n",
    "            downsample_slope_geology_vs30_grid(bbox, num, weighting,\n",
    "                                               averaging=averaging,\n",
    "                                               geol_weighting=geol_weighting,\n",
    "                                               default_xvf=default_xvf,\n",
    "                                               n=smooth_n,\n",
    "                                               onshore_only=onshore_only,\n",
    "                                               as_dataframe=as_dataframe)\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def site_model_from_exposure_model(cls, admin_model, admin_level,\n",
    "                                       weighting=True, averaging=\"MEAN\",\n",
    "                                       **kwargs):\n",
    "        \"\"\"\n",
    "        Defines the site model from a set of polygons in the form of a\n",
    "        geopandas.GeoDataFrame, taking the \"average\" site property across\n",
    "        the entire polygon\n",
    "        \"\"\"\n",
    "        geol_weighting = kwargs.get(\"geol_weighting\", \"MODE\")\n",
    "        default_xvf = kwargs.get(\"default_xvf\", 150.0)\n",
    "        smooth_n = kwargs.get(\"smooth_n\", 0)\n",
    "        # Build exposure geodataframe\n",
    "        print(\"---- Calculating site properties\")\n",
    "        admin_model = get_slope_geology_vs30_polygons(\n",
    "            admin_model, \"ID_{:g}\".format(admin_level), weighting,\n",
    "            method=averaging, method_geol=geol_weighting, n=smooth_n,\n",
    "            proj=EUROPE_EQUAL_AREA)\n",
    "        # Get the backarc distances\n",
    "        print(\"---- Getting backarc distance\")\n",
    "        admin_model[\"xvf\"] = interpolate_xvf_grid(\n",
    "            admin_model[\"lon\"].values, admin_model[\"lat\"].values,\n",
    "            default_xvf=default_xvf)\n",
    "        # Should now have a geometry (polygon), lons, lats, vs30s, geologies,\n",
    "        # slopes and backarc distances - so basically a fully formed site\n",
    "        # model\n",
    "        return cls(admin_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def site_model_from_points(cls, lons, lats, **kwargs):\n",
    "        \"\"\"\n",
    "        Defines the site model by assigning to each location the site property\n",
    "        at the exact location of the site\n",
    "        \"\"\"\n",
    "        onshore_only = kwargs.get(\"onshore_only\", True)\n",
    "        site_model = get_slope_geology_vs30_at_location(\n",
    "            lons, lats, spc=(1. / 120.),\n",
    "            onshore_only=onshore_only,\n",
    "            as_dataframe=True)\n",
    "        return cls(site_model, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def site_model_from_admin_zones(cls, admin_model, admin_level,\n",
    "                                    weighting=True, centroid=\"MEAN\",\n",
    "                                    averaging=\"MEAN\", **kwargs):\n",
    "        \"\"\"\n",
    "        Similar to the case of the exposure model; however the starting point\n",
    "        is a set of polygons without associated sites, and thus the centroids\n",
    "        need to be defined.\n",
    "        \"\"\"\n",
    "        geol_weighting = kwargs.get(\"geol_weighting\", \"MODE\")\n",
    "        default_xvf = kwargs.get(\"default_xvf\", 150.0)\n",
    "        smooth_n = kwargs.get(\"smooth_n\", 0)\n",
    "        print(\"---- Determining centroids\")\n",
    "        admin_model = get_weighted_centroids_of_polygon_wgs84(\n",
    "            admin_model, admin_level, centroid_method=centroid, n=smooth_n)\n",
    "        print(\"---- Getting site properties\")\n",
    "        admin_model = get_slope_geology_vs30_polygons(\n",
    "            admin_model, \"ID_{:g}\".format(admin_level), weighting,\n",
    "            method=averaging, method_geol=geol_weighting, n=smooth_n,\n",
    "            proj=EUROPE_EQUAL_AREA)\n",
    "        print(\"---- Getting backarc distance\")\n",
    "        admin_model[\"xvf\"] = interpolate_xvf_grid(\n",
    "            admin_model[\"lon\"].values, admin_model[\"lat\"].values,\n",
    "            default_xvf=default_xvf)\n",
    "        return cls(admin_model, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_amplification_model(bbox, spacing, imts, reference_slope,\n",
    "                                  reference_geology, skip_unknown=True,\n",
    "                                  **kwargs):\n",
    "        \"\"\"\n",
    "        Builds model of amplification for a given region defined by a bounding\n",
    "        box\n",
    "\n",
    "        :param list bbox:\n",
    "            Bounding box to define the region [east, south, west, north]\n",
    "        :param float spacing:\n",
    "            Spacing of the model in arc-seconds (must be a multiple of 30)\n",
    "        :param list imts:\n",
    "            Intensity measure types as a list containing either \"pga\" or\n",
    "            float values representing the periods, e.g. [\"pga\", 0.1, 0.2, 1.0]\n",
    "        :param float reference_slope:\n",
    "            Slope value (in m/m) to be adopted as the reference slope\n",
    "        :param str reference_geology:\n",
    "            Geological unit to be adopted as the reference geology\n",
    "        :param bool skip_unknown:\n",
    "            For sites with an unknown geological unit (including offshore)\n",
    "            return a NaN\n",
    "        **kwargs = Other arguments relevant for downsampling passed to\n",
    "                   downsample_slope_geology_vs30_grid\n",
    "\n",
    "        :returns:\n",
    "            Amplification model as a dictionary containing:\n",
    "            'amplification': 3D grid of amplification [nlat, nlon, num. imts]\n",
    "            'phis2s': Site-to-site standard deviation (phis2s) as a list of\n",
    "                      length len(imts)\n",
    "            'lons': Vector of longitudes\n",
    "            'lats': Vector of latitudes\n",
    "            'slope': 2D array of slope values (in m/m)\n",
    "            'geology': 2D numpy array of geological units\n",
    "            'bbox': Bounding box information\n",
    "            'spacing': spacing information\n",
    "            'imts': List of intensity measured\n",
    "        \"\"\"\n",
    "        assert bbox[2] > bbox[0]\n",
    "        assert bbox[3] > bbox[1]\n",
    "        if (spacing % 30):\n",
    "            raise ValueError(\"Spacing must be a multiple of 30\")\n",
    "        num = int(spacing) // 30\n",
    "        if num == 1:\n",
    "            # Extracting the 30 arc-second model so no downsampling is needed\n",
    "            lons, lats, slope = slice_wgs84_datafile(SITE_HDF5_FILE, bbox, \"slope\")\n",
    "            geology_code = slice_wgs84_datafile(SITE_HDF5_FILE, bbox, \"geology\")[2]\n",
    "\n",
    "        else:\n",
    "            # Downsampling the 30 arc second grid to a lower resolution\n",
    "            weighting = kwargs.get(\"weighting\", True)\n",
    "            averaging = kwargs.get(\"averaging\", \"MEAN\")\n",
    "            geol_weighting = kwargs.get(\"geol_weighting\", \"MODE\")\n",
    "            default_xvf = kwargs.get(\"default_xvf\", 150.0)\n",
    "            smooth_n = kwargs.get(\"smooth_n\", 0)\n",
    "            slope, vs30s, geology_code, xvf, lons, lats, idx =\\\n",
    "                downsample_slope_geology_vs30_grid(bbox, num, weighting,\n",
    "                                                   averaging=averaging,\n",
    "                                                   geol_weighting=geol_weighting,\n",
    "                                                   default_xvf=default_xvf,\n",
    "                                                   n=smooth_n,\n",
    "                                                   onshore_only=False,\n",
    "                                                   as_dataframe=False)\n",
    "        # Convert geology from the code to the expected bytestrings\n",
    "        geology = np.zeros(geology_code.shape, dtype=(np.string_, 20))\n",
    "        for key in GEOL_DICT_KEY_TO_NAME:\n",
    "            idx = geology_code == key\n",
    "            if np.any(idx):\n",
    "                geology[idx] = GEOL_DICT_KEY_TO_NAME[key]\n",
    "        ampls, phis2s = site_amp_model_slope_geology(\n",
    "            imts, slope, geology,\n",
    "            reference_slope=reference_slope,\n",
    "            reference_geology=reference_geology,\n",
    "            skip_unknown=True)\n",
    "        for i, (imt, phis2s_i) in enumerate(zip(imts, phis2s)):\n",
    "            print(\"IMT: %s    PHI_S2S = %.6f\" % (str(imt), phis2s_i))\n",
    "        return {\"amplification\": ampls, \"phis2s\": phis2s, \"lons\": lons,\n",
    "                \"lats\": lats, \"slope\": slope, \"geology\": geology,\n",
    "                \"bbox\": bbox, \"spacing\": spacing, \"imts\": imts}\n",
    "\n",
    "    @staticmethod\n",
    "    def export_amplification_model(output_folder, model, filetype=\"tif\"):\n",
    "        \"\"\"\n",
    "        Exports an amplifiction model to a set of 2D raster files\n",
    "\n",
    "        :param str output_folder:\n",
    "            Path to output folder for export (will be created if it doesn't exist\n",
    "        :param dict model:\n",
    "            Amplification model as output from SiteManager.build_amplification_model\n",
    "        :param str filetype:\n",
    "            File type to export the rasters: geotiff ('tif') or ESRI arc-ascii ('asc')\n",
    "        \"\"\"\n",
    "        spc = float(int(model[\"spacing\"]) // 30) * (1.0 / 120.0)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.mkdir(output_folder)\n",
    "        if filetype == \"tif\":\n",
    "            export_amplification_maps_to_geotiff(\n",
    "                output_folder,\n",
    "                model[\"amplification\"],\n",
    "                model[\"imts\"],\n",
    "                model[\"bbox\"],\n",
    "                spc, spc\n",
    "                )\n",
    "        elif filetype == \"asc\":\n",
    "            export_amplification_maps_to_ascii(\n",
    "                output_folder,\n",
    "                model[\"amplification\"],\n",
    "                model[\"imts\"],\n",
    "                model[\"bbox\"],\n",
    "                spc, spc\n",
    "                )\n",
    "        else:\n",
    "            raise OSError(\"File type for export %s not recognised\" % filetype)\n",
    "        return\n",
    "\n",
    "    def replace_geometry_for_lonlat(self, output_model):\n",
    "        \"\"\"\n",
    "        Replaces the Point geometry with the longitude and latitude\n",
    "        \"\"\"\n",
    "        lonlat = np.array([[geom.x, geom.y]\n",
    "                           for geom in self.site_model.geometry.values])\n",
    "        output_model[\"lon\"] = lonlat[:, 0]\n",
    "        output_model[\"lat\"] = lonlat[:, 1]\n",
    "        # Remove the original geometry (makes the output lighter)\n",
    "        del output_model[\"geometry\"]\n",
    "        return output_model\n",
    "\n",
    "    def add_backarc_distances_to_site_model(self, default_xvf=150.):\n",
    "        \"\"\"\n",
    "        Retrieves the volcanic front distance for the target sites\n",
    "        \"\"\"\n",
    "        if \"xvf\" in self.site_model.columns:\n",
    "            return\n",
    "        self.site_model[\"xvf\"] =\\\n",
    "            interpolate_xvf_grid(\n",
    "                self.site_model[\"lon\"].values, self.site_model[\"lat\"].values,\n",
    "                default_xvf=default_xvf)\n",
    "\n",
    "    def replace_lonlat_for_geometry(self, output_model):\n",
    "        \"\"\"\n",
    "        Replaces the longitude and latitude with the geometry\n",
    "        \"\"\"\n",
    "        output_model[\"geometry\"] = gpd.GeoSeries(\n",
    "           [Point(lon, lat) for lon, lat in zip(self.site_model[\"lon\"].values,\n",
    "                                                self.site_model[\"lat\"].values)])\n",
    "        # Delete the lon, lat\n",
    "        del output_model[\"lon\"]\n",
    "        del output_model[\"lat\"]\n",
    "        return output_model\n",
    "\n",
    "    def insert_default_column(self, output_model, default_key):\n",
    "        \"\"\"\n",
    "        Adds a column of default values to the site model\n",
    "        \"\"\"\n",
    "        default_val = self.default_conditions[default_key]\n",
    "        if isinstance(default_val, str):\n",
    "            # Add the list of strings\n",
    "            output_model[default_key] =\\\n",
    "                [default_val for i in range(self.site_model.shape[0])]\n",
    "        else:\n",
    "            # Is numerical - add a numpy array\n",
    "            output_model[default_key] =\\\n",
    "                default_val * np.ones(self.site_model.shape[0],\n",
    "                                      dtype=default_val.__class__)\n",
    "\n",
    "    def to_csv(self, filename, sep=\",\"):\n",
    "        \"\"\"\n",
    "        Exports the site model to a csv format\n",
    "        \"\"\"\n",
    "        output_model = self.site_model.copy()\n",
    "        # Check headers are correct\n",
    "        invalid_points = np.zeros(output_model.shape[0], dtype=int)\n",
    "        for col in self.REQUIRED_HEADERS:\n",
    "            if col not in self.site_model.columns:\n",
    "                if col in (\"lon\", \"lat\"):\n",
    "                    # Lon/Lat not in site model - can retrieve it from geometry\n",
    "                    output_model = self.replace_geometry_for_lonlat(\n",
    "                        output_model)\n",
    "                elif col in (\"vs30measured\",):\n",
    "                    self.insert_default_column(output_model, \"vs30measured\")\n",
    "                else:\n",
    "                    raise ValueError(\"Site model missing attribute %s\" % col)\n",
    "            # Check for invalid values, i.e. nans\n",
    "            invalid_points += output_model[col].isnull().values.astype(int)\n",
    "        idx = invalid_points > 0\n",
    "        if np.any(invalid_points):\n",
    "            # Purge sites with nans in the required attributes\n",
    "            output_model = output_model[np.logical_not(idx)]\n",
    "        # Drop duplicates\n",
    "        output_model.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        print(\"Exporting model to csv file %s\" % filename)\n",
    "        if \"geometry\" in output_model.columns:\n",
    "            # Remove geometry\n",
    "            del output_model[\"geometry\"]\n",
    "        output_model.to_csv(filename, sep=sep, index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_csv(cls, filename, sep=\",\", **kwargs):\n",
    "        \"\"\"\n",
    "        Builds a site model object from a csv file\n",
    "        \"\"\"\n",
    "        site_model_input = pd.read_csv(filename, sep=\",\")\n",
    "        site_model = gpd.GeoDataFrame({\n",
    "            \"geometry\": gpd.GeoSeries(\n",
    "                [Point(lon, lat) for lon, lat in zip(site_model_input[\"lon\"].values,\n",
    "                                                     site_model_input[\"lat\"].values)]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for key in cls.REQUIRED_HEADERS:\n",
    "            if key not in site_model_input.columns:\n",
    "                print(\"Required attribute %s missing from file - may not \"\n",
    "                      \"function as needed\" % key)\n",
    "            else:\n",
    "                site_model[key] = site_model_input[key]\n",
    "        return cls(site_model, **kwargs)\n",
    "\n",
    "    def to_shapefile(self, filename):\n",
    "        \"\"\"\n",
    "        Exports the site model to shapefile\n",
    "        \"\"\"\n",
    "        output_model = self.site_model.copy()\n",
    "        for col in self.REQUIRED_HEADERS:\n",
    "            if col not in self.site_model.columns:\n",
    "                if col in (\"vs30measured\",):\n",
    "                    self.insert_default_column(output_model, \"vs30measured\")\n",
    "                else:\n",
    "                    raise ValueError(\"Site model missing attribute %s\" % col)\n",
    "\n",
    "        if \"geometry\" not in self.site_model.columns:\n",
    "            output_model = self.replace_lonlat_for_geometry(output_model)\n",
    "\n",
    "        print(\"Exporting site model to shapefile %s\" % filename)\n",
    "        output_model.to_file(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def from_shapefile(cls, filename, **kwargs):\n",
    "        \"\"\"\n",
    "        Loads in the site model from a shapefile\n",
    "        \"\"\"\n",
    "        input_model = gpd.GeoDataFrame.from_file(filename)\n",
    "        has_lonlat = (\"lon\" in input_model.columns) and\\\n",
    "            (\"lat\" in input_model.columns)\n",
    "        if not has_lonlat:\n",
    "            # Longitude and latitude columns missing; take them from geometry\n",
    "            lons, lats = zip(*[(pnt.x, pnt.y)\n",
    "                             for pnt in input_model.geometry.values])\n",
    "            input_model[\"lon\"] = np.array(lons)\n",
    "            input_model[\"lat\"] = np.array(lats)\n",
    "\n",
    "        for key in cls.REQUIRED_HEADERS:\n",
    "            if key not in input_model.columns:\n",
    "                print(\"Required attribute %s missing from file - may not \"\n",
    "                      \"function as needed\" % key)\n",
    "        return cls(input_model, **kwargs)\n",
    "\n",
    "    def to_xml(self, filename, fmt=\"%s\"):\n",
    "        \"\"\"\n",
    "        Exports the site model to a site model xml file\n",
    "        \"\"\"\n",
    "        output_model = self.site_model.copy()\n",
    "        invalid_points = np.zeros(output_model.shape[0], dtype=int)\n",
    "        # Check headers are correct\n",
    "        for col in self.REQUIRED_HEADERS:\n",
    "            if col not in self.site_model.columns:\n",
    "                if col in (\"lon\", \"lat\"):\n",
    "                    # Lon/Lat not in site model - can retrieve it from geometry\n",
    "                    output_model = self.replace_geometry_for_lonlat(\n",
    "                        output_model)\n",
    "                elif col in (\"vs30measured\",):\n",
    "                    self.insert_default_column(output_model, \"vs30measured\")\n",
    "                else:\n",
    "                    raise ValueError(\"Site model missing attribute %s\" % col)\n",
    "            # Check for invalid values, i.e. nans\n",
    "            invalid_points += output_model[col].isnull().values.astype(int)\n",
    "        idx = invalid_points > 0\n",
    "        if np.any(invalid_points):\n",
    "            # Purge sites with nans in the required attributes\n",
    "            output_model = output_model[np.logical_not(idx)]\n",
    "        # Drop duplicate sites\n",
    "        output_model.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        nodes = []\n",
    "        print(\"Building site_model for export\")\n",
    "        for i in range(output_model.shape[0]):\n",
    "            attribs = {\"lon\": output_model[\"lon\"].values[i],\n",
    "                       \"lat\": output_model[\"lat\"].values[i],\n",
    "                       \"vs30\": output_model[\"vs30\"].values[i],\n",
    "                       \"slope\": output_model[\"slope\"].values[i],\n",
    "                       \"geology\": output_model[\"geology\"].values[i],\n",
    "                       \"xvf\": output_model[\"xvf\"].values[i],\n",
    "                       \"region\": output_model[\"region\"].values[i]}\n",
    "            if output_model[\"vs30measured\"].values[i]:\n",
    "                attribs[\"vs30Type\"] = \"measured\"\n",
    "            else:\n",
    "                attribs[\"vs30Type\"] = \"inferred\"\n",
    "\n",
    "            nodes.append(Node(\"site\", attrib=attribs))\n",
    "        site_model = Node(\"siteModel\", nodes=nodes)\n",
    "        print(\"Exporting site model to xml file %s\" % filename)\n",
    "        with open(filename, \"wb\") as f:\n",
    "            nrml_write([site_model], f, fmt=fmt)\n",
    "\n",
    "    @classmethod\n",
    "    def from_xml(cls, filename, **kwargs):\n",
    "        \"\"\"\n",
    "        Builds a site model object from a nrml file\n",
    "        \"\"\"\n",
    "        [site_input] = nrml_read(filename)\n",
    "        site_model = {}\n",
    "        for node in site_input:\n",
    "            if not site_model:\n",
    "                for key in node.attrib:\n",
    "                    if key in cls.STRING_ATTRIBS:\n",
    "                        site_model[key] = [node[key]]\n",
    "                    elif key == \"vs30Type\":\n",
    "                        if node[key] == \"measured\":\n",
    "                            site_model[\"vs30measured\"] = [1]\n",
    "                        else:\n",
    "                            site_model[\"vs30measured\"] = [0]\n",
    "\n",
    "                    else:\n",
    "                        site_model[key] = [float(node[key])]\n",
    "            else:\n",
    "                for key in node.attrib:\n",
    "                    if key in cls.STRING_ATTRIBS:\n",
    "                        site_model[key].append(node[key])\n",
    "                    elif key == \"vs30Type\":\n",
    "                        if node[key] == \"measured\":\n",
    "                            site_model[\"vs30measured\"].append(1)\n",
    "                        else:\n",
    "                            site_model[\"vs30measured\"].append(0)\n",
    "                    else:\n",
    "                        site_model[key].append(float(node[key]))\n",
    "        for key in site_model:\n",
    "            if key not in cls.STRING_ATTRIBS:\n",
    "                site_model[key] = np.array(site_model[key])\n",
    "        site_model[\"geometry\"] = gpd.GeoSeries(\n",
    "            [Point(lon, lat) for lon, lat in zip(site_model[\"lon\"],\n",
    "                                                 site_model[\"lat\"])]\n",
    "        )\n",
    "        return cls(gpd.GeoDataFrame(site_model), **kwargs)\n",
    "\n",
    "\n",
    "def set_up_arg_parser():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    description = (\"Builds the site models needed for SERA seismic risk \"\n",
    "                   \"calculations, from the 30 arc second grids. To run type: \\n\"\n",
    "                   \"\\n\"\n",
    "                   \">> python exposure_to_site_tools.py --run CALC_TYPE \"\n",
    "                   \"--output-file PATH/TO/OUTPUT_FILE\\n\"\n",
    "                   \"\\n\"\n",
    "                   \"Other arguments are specific to the type of calculator\\n\")\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=description, add_help=True,\n",
    "        formatter_class=argparse.RawTextHelpFormatter)\n",
    "    parser.add_argument('-r', '--run',\n",
    "                        choices=[\"grid\", \"admin\", \"exposure\", \"point\", \"join\",\n",
    "                                 \"amplification\"],\n",
    "                        help=\"Type of configuration ('grid', \"\n",
    "                        \"'admin', 'exposure', 'point', 'join', 'amplification')\",\n",
    "                        required=True)\n",
    "    parser.add_argument('-of', '--output-file', help='Path to output file',\n",
    "                        required=True)\n",
    "\n",
    "    # Options applicable to many groups\n",
    "    parser.add_argument('-w', '--weighting', help=\"Use building density to weight \"\n",
    "                        \"the values within an area/cell (default True)\",\n",
    "                        required=False, default=\"True\")\n",
    "    parser.add_argument('-a', '--averaging', help=\"Defines how to take assign a \"\n",
    "                        \"value within a cell or polygon ('mean', 'median', max')\",\n",
    "                        choices=[\"mean\", \"median\", \"max\"],\n",
    "                        required=False, default=\"mean\")\n",
    "    parser.add_argument('-gw', '--geological-weighting', help=\"Get the weighted \"\n",
    "                        \"median geological category (or else the modal \"\n",
    "                        \"category)\", choices=[\"mode\", \"median\", \"max\"],\n",
    "                        required=False, default=\"mode\")\n",
    "    parser.add_argument('-dxvf', '--default-xvf',\n",
    "                        help=\"Default distance to the volcanic front  (km)\",\n",
    "                        required=False, default=\"150.0\")\n",
    "    parser.add_argument('-oo', '--onshore-only', help=\"Clip the grid sites to \"\n",
    "                        \"only those onshore\", required=False,\n",
    "                        default=\"True\")\n",
    "    parser.add_argument('-sn', '--smooth-n', help=\"N-th order neighbourhood to\"\n",
    "                        \" use for weighting\", required=False, default=\"0\")\n",
    "\n",
    "    # Arguments specific to the regular grid option\n",
    "    grid_group = parser.add_argument_group(\n",
    "        \"grid\",\n",
    "        \"Renders site model on a regularly-spaced, weighted grid\")\n",
    "    grid_group.add_argument('--bbox', help=\"Bounding Box LLON/LLAT/ULON/ULAT\",\n",
    "                            required=False, default=None)\n",
    "    grid_group.add_argument('-s', '--spacing', help=\"Spacing of downsampled grid \"\n",
    "                            \"in arc seconds (must be a multiple of 30)\",\n",
    "                            required=False, default=\"30\")\n",
    "\n",
    "    # Arguments specific to the exposure model option\n",
    "    exposure_group = parser.add_argument_group(\n",
    "        \"exposure\",\n",
    "        \"Builds site model by averaging across polygons in an exposure model\")\n",
    "    exposure_group.add_argument('-iexp', '--input-exposure',\n",
    "                                help=\"Path to input csv exposure file\",\n",
    "                                required=False, default=None)\n",
    "    exposure_group.add_argument('-sd', '--shapefile-dir',\n",
    "                                help=\"Path to directory containing shapefiles\",\n",
    "                                required=False, default=None)\n",
    "    exposure_group.add_argument('-tal', \"--target-admin-level\",\n",
    "                                help=\"Target admin level for the exposure \"\n",
    "                                \"- highest in exposure file if not specified\",\n",
    "                                required=False, default=None)\n",
    "\n",
    "    # Arguments specific to the admin regions option\n",
    "    admin_group = parser.add_argument_group(\n",
    "        \"admin\",\n",
    "        \"Builds a site model for a set of admin regions, including definition \"\n",
    "        \"of the centroid\")\n",
    "    admin_group.add_argument('-ishp', \"--input-shapefile\",\n",
    "                             help=\"Path to input admin shapefile\",\n",
    "                             required=False)\n",
    "    admin_group.add_argument('-al', \"--admin-level\",\n",
    "                             help=\"Administrative level for aggregation (will \"\n",
    "                             \"take the highest found if not specified)\",\n",
    "                             required=False, default=\"\")\n",
    "    admin_group.add_argument('-cw', \"--centroid-weighting\",\n",
    "                             help=\"Method for weighting the centroid of the \"\n",
    "                             \"admin region according to building density\",\n",
    "                             choices=[\"mean\", \"median\", \"max\"],\n",
    "                             required=False, default=\"\")\n",
    "\n",
    "    # Arguments specific to the point model option\n",
    "    point_group = parser.add_argument_group(\n",
    "        \"point\",\n",
    "        \"Builds site model from a set of locations by taking the property \"\n",
    "        \"value directly at the location\"\n",
    "        )\n",
    "    point_group.add_argument('-if', \"--input-file\",\n",
    "                             help=\"Path to input text/csv file\",\n",
    "                             required=False)\n",
    "\n",
    "    # Arguments specific to the join model option\n",
    "    join_group = parser.add_argument_group(\n",
    "        \"join\",\n",
    "        \"Join together multiple site model files or a directory of files\"\n",
    "        )\n",
    "    join_group.add_argument('-id', '--input-dir',\n",
    "                            help=\"Path to directory of input files\")\n",
    "    join_group.add_argument('-f', '--files', help=\"+ delimited list of files \"\n",
    "                            \"(e.g. file1.csv+file2.csv+...)\")\n",
    "\n",
    "    # Arguments specific to the amplification map option\n",
    "    ampl_group = parser.add_argument_group(\n",
    "        \"amplification\",\n",
    "        \"Builds a map of expected amplification for a given region \"\n",
    "        \"(requires `bbox` and `spacing` arguments as for the `grid` workflow\"\n",
    "        )\n",
    "    ampl_group.add_argument('-imts', '--imts', help=\"Comma separated list of intensity measure \"\n",
    "                            \"types as either 'pga' or spectral period, e.g. for PGA, SA(0.1s),\"\n",
    "                            \" SA(1.0 s) input 'pga,0.1,1.0\", required=True)\n",
    "    ampl_group.add_argument('-rs', '--reference-slope', help=\"Reference slope condition (m/m)\",\n",
    "                            required=True)\n",
    "    ampl_group.add_argument('-rg', '--reference-geology', help=\"Reference geological unit \"\n",
    "                            \"(must be one from the following: %s)\" % str(GEOLOGICAL_UNITS),\n",
    "                            required=True)\n",
    "    ampl_group.add_argument('-ft', '--file-type', help=\"File type to export amplification \"\n",
    "                            \"map: either Geotiff ('tif') or ESRI Arc-Ascii ('asc')\",\n",
    "                            required=False, default=\"tif\")\n",
    "\n",
    "    # Return arguments\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = set_up_arg_parser()\n",
    "    # Argparser - arguments\n",
    "    args = parser.parse_args()\n",
    "    # Check if output file already exists and raise error if so\n",
    "    if os.path.exists(args.output_file):\n",
    "        raise IOError(\"Output file %s already exists!\" % args.output_file)\n",
    "\n",
    "    if args.run == \"amplification\":\n",
    "        os.mkdir(args.output_file)\n",
    "    else:\n",
    "        # Define output file type and check they are one of \"csv\", \"xml\" or \"sha\"\n",
    "        output_file_type = os.path.splitext(args.output_file)[1]\n",
    "        assert output_file_type in (\".csv\", \".xml\", \".shp\")\n",
    "\n",
    "    if args.run == \"grid\":\n",
    "        # ----------------- Workflow 1 ----------------------------------------\n",
    "        # From a bounding box get a downsampled grid at a multiple of 30 arc\n",
    "        # seconds\n",
    "        bbox = list(map(float, args.bbox.split(\"/\")))\n",
    "        spacing = int(args.spacing)\n",
    "        weighting = bool(args.weighting)\n",
    "        averaging = args.averaging.upper()\n",
    "        onshore_only = bool(args.onshore_only)\n",
    "        geol_weighting = args.geological_weighting.upper()\n",
    "        def_xvf = float(args.default_xvf)\n",
    "        smooth_n = int(args.smooth_n)\n",
    "        # Build the site model\n",
    "        site_manager = SiteManager.site_model_from_bbox(\n",
    "            bbox, spacing, weighting, averaging=averaging, geol_weighting=geol_weighting,\n",
    "            default_xvf=def_xvf, onshore_only=onshore_only, smooth_n=smooth_n)\n",
    "\n",
    "    elif args.run == \"exposure\":\n",
    "        # ---------------- Workflow 2 -----------------------------------------\n",
    "        # From an input exposure model file and a path to the corresponding\n",
    "        # geometries in the shapefile, constructs a site model by averaging the\n",
    "        # properties across the administrative polygon\n",
    "        weighting = args.weighting\n",
    "        averaging = args.averaging.upper()\n",
    "        onshore_only = bool(args.onshore_only)\n",
    "        geol_weighting = args.geological_weighting.upper()\n",
    "        def_xvf = float(args.default_xvf)\n",
    "        smooth_n = int(args.smooth_n)\n",
    "\n",
    "        # Get the admin model (as a geodataframe) and the admin level from\n",
    "        # the exposure model\n",
    "        print(\"Retrieving exposure model from %s\" % args.input_exposure)\n",
    "        admin_model, admin_level = get_site_set_from_exposure(\n",
    "            args.input_exposure, args.shapefile_dir,\n",
    "            args.target_admin_level)\n",
    "        print(\"Building site model\")\n",
    "        site_manager = SiteManager.site_model_from_exposure_model(\n",
    "            admin_model, admin_level, weighting, averaging=averaging,\n",
    "            geol_weighting=geol_weighting, default_xvf=def_xvf,\n",
    "            smooth_n=smooth_n)\n",
    "\n",
    "    elif args.run == \"admin\":\n",
    "        # --------------- Workflow 3 ------------------------------------------\n",
    "        # From a set of administrative boundaries defines the site model,\n",
    "        # including the determination of the target sites from the [weighted]\n",
    "        # centroids of each admin zone\n",
    "\n",
    "        # Setup general properties\n",
    "        weighting = args.weighting\n",
    "        averaging = args.averaging.upper()\n",
    "        geol_weighting = args.geological_weighting.upper()\n",
    "        def_xvf = float(args.default_xvf)\n",
    "        centroid_weighting = args.centroid_weighting.upper()\n",
    "        smooth_n = int(args.smooth_n)\n",
    "        # Admin level\n",
    "        if args.admin_level:\n",
    "            admin_level = int(args.admin_level)\n",
    "            # Check this is in the model\n",
    "        else:\n",
    "            admin_level = None\n",
    "\n",
    "        # Load in data from file\n",
    "        print(\"Loading admin model %s\" % args.input_shapefile)\n",
    "        admin_model = gpd.GeoDataFrame.from_file(args.input_shapefile)\n",
    "        # Just in case the ID and NAME columns are in lower case, render them\n",
    "        # all to upper case\n",
    "        admin_model = admin_model.rename(KEY_MAPPER, axis='columns')\n",
    "        if admin_level:\n",
    "            # Admin level requested but not found in shapefile\n",
    "            if not \"ID_{:g}\".format(admin_level) in admin_model.columns:\n",
    "                raise IOError(\"Required admin level %g not defined in \"\n",
    "                              \"shapefile %s\" % (admin_level,\n",
    "                                                args.input_shapefile))\n",
    "        else:\n",
    "            admin_level = get_maximum_admin_level(admin_model)\n",
    "            print(\"Highest admin level found = %g\" % admin_level)\n",
    "        # Now setup the site model\n",
    "        print(\"Building site model\")\n",
    "        site_manager = SiteManager.site_model_from_admin_zones(\n",
    "            admin_model, admin_level, weighting, centroid_weighting, averaging,\n",
    "            geol_weighting=geol_weighting, default_xvf=def_xvf,\n",
    "            smooth_n=smooth_n)\n",
    "\n",
    "    elif args.run == \"point\":\n",
    "        # --------------- Workflow 4 ------------------------------------------\n",
    "        # For a set of longitudes and latitudes simply retrieve the properties\n",
    "        # at the locations of the longitudes and latitudes\n",
    "\n",
    "        onshore_only = bool(args.onshore_only)\n",
    "        print(\"Loading locations from %s\" % args.input_file)\n",
    "        df = pd.read_csv(args.input_file, sep=\",\")\n",
    "        has_lonlat = (\"lon\" in df.columns) and (\"lat\" in df.columns)\n",
    "        if not has_lonlat:\n",
    "            # Check if inputs in xcoord, ycoord\n",
    "            has_crds = (\"xcoord\" in df.columns) and (\"ycoord\" in df.columns)\n",
    "            if has_crds:\n",
    "                df[\"lon\"] = df[\"xcoord\"].values\n",
    "                df[\"lat\"] = df[\"ycoord\"].values\n",
    "            else:\n",
    "                raise ValueError(\"Input file %s has no lon,lat or \"\n",
    "                                 \"xcoord,ycoord attributes\" % args.input_file)\n",
    "        # Dropping duplicate sites\n",
    "        df.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        print(\"Building site model for points\")\n",
    "        site_manager = SiteManager.site_model_from_points(\n",
    "            df[\"lon\"].values, df[\"lat\"].values, onshore_only=onshore_only)\n",
    "\n",
    "    elif args.run == \"join\":\n",
    "        if args.input_dir and os.path.isdir(args.input_dir):\n",
    "            files = []\n",
    "            for filename in os.listdir(args.input_dir):\n",
    "                if filename.endswith(\".csv\") or filename.endswith(\".xml\"):\n",
    "                    files.append(os.path.join(args.input_dir, filename))\n",
    "            if not len(files):\n",
    "                raise ValueError(\"No csv or xml files found in %s\"\n",
    "                                 % args.input_dir)\n",
    "        else:\n",
    "            files = args.files.split(\"+\")\n",
    "        assert len(files) > 1\n",
    "\n",
    "        if files[0].endswith(\"csv\"):\n",
    "            site_manager = SiteManager.from_csv(files[0])\n",
    "        elif files[0].endswith(\"xml\"):\n",
    "            site_manager = SiteManager.from_xml(files[0])\n",
    "        else:\n",
    "            raise ValueError(\"File %s not csv or xml\" % files[0])\n",
    "        for filename in files[1:]:\n",
    "            if filename.endswith(\"csv\"):\n",
    "                temp_manager = SiteManager.from_csv(filename)\n",
    "            elif filename.endswith(\"xml\"):\n",
    "                temp_manager = SiteManager.from_xml(filename)\n",
    "            else:\n",
    "                raise ValueError(\"File %s not csv or xml\" % filename)\n",
    "            site_manager += temp_manager\n",
    "\n",
    "    elif args.run == \"amplification\":\n",
    "        bbox = list(map(float, args.bbox.split(\"/\")))\n",
    "        spacing = int(args.spacing)\n",
    "        weighting = bool(args.weighting)\n",
    "        averaging = args.averaging.upper()\n",
    "        onshore_only = bool(args.onshore_only)\n",
    "        geol_weighting = args.geological_weighting.upper()\n",
    "        def_xvf = float(args.default_xvf)\n",
    "        smooth_n = int(args.smooth_n)\n",
    "\n",
    "        imts = [imt if imt in (\"pga\", \"pgv\") else float(imt)\n",
    "                for imt in args.imts.split(\",\")]\n",
    "\n",
    "        amplification = SiteManager.build_amplification_model(\n",
    "            bbox, spacing, imts, float(args.reference_slope),\n",
    "            args.reference_geology, skip_unknown=onshore_only, weighting=weighting,\n",
    "            averaging=averaging, geol_weighting=geol_weighting,\n",
    "            default_xvf=def_xvf, smooth_n=smooth_n)\n",
    "        SiteManager.export_amplification_model(args.output_file,\n",
    "                                               amplification,\n",
    "                                               args.file_type)\n",
    "        return\n",
    "    else:\n",
    "        # Shouldn't really have reached this point, but raising an IOError\n",
    "        # just in case\n",
    "        raise IOError(\"Run type %s not recognised\" % args.run)\n",
    "    print(\"Exporting to file %s\" % args.output_file)\n",
    "    # Export\n",
    "    if output_file_type == \".xml\":\n",
    "        # Export to xml\n",
    "        site_manager.to_xml(args.output_file)\n",
    "    elif output_file_type == \".shp\":\n",
    "        site_manager.to_shapefile(args.output_file)\n",
    "    else:\n",
    "        # Export to csv\n",
    "        site_manager.to_csv(args.output_file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6404822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate reference systems\n",
    "CRS_WGS84 = CRS.from_epsg(4326)\n",
    "MOLL_STR = '+ellps=WGS84 +lon_0=0 +no_defs +proj=moll +units=m +x_0=0 +y_0=0'\n",
    "CRS_MOLL = CRS.from_string(MOLL_STR)\n",
    "EUROPE_EQUAL_AREA = \"epsg:3035\"\n",
    "SMOOTHED_N = [1, 3, 5]\n",
    "# Transformations between Molleweide and WGS84\n",
    "TRANSFORMER = Transformer.from_crs(CRS_WGS84, CRS_MOLL, always_xy=True)\n",
    "TRANSFORMER_REV = Transformer.from_crs(CRS_MOLL, CRS_WGS84, always_xy=True)\n",
    "\n",
    "# Geological eras stored as integers - relates the geological era to the int\n",
    "GEOL_DICT_KEY_TO_NAME = {0: \"UNKNOWN\",\n",
    "                         1: \"PRECAMBRIAN\",\n",
    "                         2: \"PALEOZOIC\",\n",
    "                         3: \"JURASSIC-TRIASSIC\",\n",
    "                         4: \"CRETACEOUS\",\n",
    "                         5: \"CENOZOIC\",\n",
    "                         6: \"PLEISTOCENE\",\n",
    "                         7: \"HOLOCENE\"}\n",
    "\n",
    "\n",
    "GEOL_DICT_NAME_TO_KEY = {\"UNKNOWN\": 0,\n",
    "                         \"PRECAMBRIAN\": 1,\n",
    "                         \"PALEOZOIC\": 2,\n",
    "                         \"JURASSIC-TRIASSIC\": 3,\n",
    "                         \"CRETACEOUS\": 4,\n",
    "                         \"CENOZOIC\": 5,\n",
    "                         \"PLEISTOCENE\": 6,\n",
    "                         \"HOLOCENE\": 7}\n",
    "\n",
    "def get_eshm20_region(self):\n",
    "        \"\"\"\n",
    "        Determines the ESHM20 attenuation region for the respective sites and\n",
    "        adds to the site model\n",
    "        \"\"\"\n",
    "        print(\"Assigning the sites to the ESHM20 regions\")\n",
    "        # Retrieve lightweight dataframe with just the geometry and re-project\n",
    "        # the cartesian reference frame\n",
    "        sm_lite = self.site_model[[\"lat\", \"lon\", \"geometry\"]].to_crs(\n",
    "            {\"init\": EUROPE_EQUAL_AREA})\n",
    "        # Get the region assignments\n",
    "        assignments = gpd.sjoin(sm_lite, self.eshm20_regions_xy, how=\"left\")\n",
    "        if assignments.shape[0] > self.site_model.shape[0]:\n",
    "            # This can happen if the region polygons are overlapping (which\n",
    "            # they shouldn't be in this case)\n",
    "            assignments.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "        # Set any nans to region 0\n",
    "        eshm20_region = assignments[\"REGION\"].values\n",
    "        eshm20_region[np.isnan(eshm20_region)] = 0.0\n",
    "        self.site_model[\"region\"] = eshm20_region.astype(int)\n",
    "        \n",
    "def site_model_from_points(lons, lats, **kwargs):\n",
    "        \"\"\"\n",
    "        Defines the site model by assigning to each location the site property\n",
    "        at the exact location of the site\n",
    "        \"\"\"\n",
    "        onshore_only = kwargs.get(\"onshore_only\", True)\n",
    "        site_model = get_slope_geology_vs30_at_location(\n",
    "            lons, lats, spc=(1. / 120.),\n",
    "            onshore_only=onshore_only,\n",
    "            as_dataframe=True)\n",
    "        return cls(site_model, **kwargs)\n",
    "    \n",
    "def get_slope_geology_vs30_at_location(site_lons, site_lats, spc=(1. / 120.),onshore_only=True, as_dataframe=False):\n",
    "    \"\"\"\n",
    "    Retrieves the slope and geology at a set of locations\n",
    "    :param np.ndarray site_lons:\n",
    "        Longitudes of target sites\n",
    "    :param np.ndarray site_lats:\n",
    "        Latitudes of target sites\n",
    "    :param float spc:\n",
    "        Spacing of reference hdf5 file with data (probably 120 arc seconds)\n",
    "    :returns:\n",
    "        site_slope: slopes of the site (in m/m)\n",
    "        site_vs30: Inferred Vs30 (in m/s)\n",
    "        site_geology: geological index of the site\n",
    "        site_backarc_distance: backarc distance (in km)\n",
    "    \"\"\"\n",
    "    # Get bounding box of sites (with spc buffer)\n",
    "    bbox = [np.min(site_lons) - spc, np.min(site_lats) - spc,\n",
    "            np.max(site_lons) + spc, np.max(site_lats) + spc]\n",
    "    # Get slope and geology from hdf5 file\n",
    "    print(\"---- Retrieving data within bounding box\")\n",
    "    lons, lats, slope = slice_wgs84_datafile(SITE_HDF5_FILE, bbox, \"slope\")\n",
    "    geology = slice_wgs84_datafile(SITE_HDF5_FILE, bbox, \"geology\")[2]\n",
    "    lons_vs, lats_vs, vs30 = slice_wgs84_datafile(VS30_HDF5_FILE, bbox, \"vs30\")\n",
    "    if onshore_only:\n",
    "        elevation = slice_wgs84_datafile(SITE_HDF5_FILE, bbox, \"elevation\")[2]\n",
    "    else:\n",
    "        elevation = np.zeros(lons.shape)\n",
    "    print(\"---- Identifying site values\")\n",
    "    # Integer number of multiples of spc indicates the x and y locations\n",
    "    dx = ((site_lons - (lons[0] - spc / 2.)) / spc).astype(int)\n",
    "    dy = (np.fabs((site_lats - (lats[0] + spc / 2.)) / spc)).astype(int)\n",
    "    dx_vs = ((site_lons - (lons_vs[0] - spc / 2.)) / spc).astype(int)\n",
    "    dy_vs = (np.fabs((site_lats - (lats_vs[0] + spc / 2.)) / spc)).astype(int)\n",
    "    # Get backarc distance\n",
    "    print(\"---- Getting volcanic distance\")\n",
    "    xvf = interpolate_xvf_grid(site_lons, site_lats)\n",
    "\n",
    "    # Return the slope and geology values at the selected locations\n",
    "    if as_dataframe:\n",
    "        print(\"---- Building dataframe\")\n",
    "        elevation = elevation[dy, dx]\n",
    "        idx = elevation >= -5.0\n",
    "        return gpd.GeoDataFrame({\n",
    "            \"geometry\": gpd.GeoSeries([\n",
    "                Point(lon, lat)\n",
    "                for lon, lat in zip(site_lons[idx], site_lats[idx])]),\n",
    "            \"lon\": site_lons[idx],\n",
    "            \"lat\": site_lats[idx],\n",
    "            \"slope\": slope[dy, dx][idx],\n",
    "            \"vs30\": vs30[dy_vs, dx_vs][idx],\n",
    "            \"geology\": [GEOL_DICT_KEY_TO_NAME[geol]\n",
    "                        for geol in geology[dy, dx][idx]],\n",
    "            \"xvf\": xvf[idx]})\n",
    "    idx = elevation >= -5.0\n",
    "    return slope[dy, dx], vs30[dy, dx], geology[dy, dx], xvf, idx\n",
    "\n",
    "def from_csv(cls, filename, sep=\",\", **kwargs):\n",
    "        \"\"\"\n",
    "        Builds a site model object from a csv file\n",
    "        \"\"\"\n",
    "        site_model_input = pd.read_csv(filename, sep=\",\")\n",
    "        site_model = gpd.GeoDataFrame({\n",
    "            \"geometry\": gpd.GeoSeries(\n",
    "                [Point(lon, lat) for lon, lat in zip(site_model_input[\"lon\"].values,\n",
    "                                                     site_model_input[\"lat\"].values)]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for key in cls.REQUIRED_HEADERS:\n",
    "            if key not in site_model_input.columns:\n",
    "                print(\"Required attribute %s missing from file - may not \"\n",
    "                      \"function as needed\" % key)\n",
    "            else:\n",
    "                site_model[key] = site_model_input[key]\n",
    "        return cls(site_model, **kwargs)\n",
    "    \n",
    "def slice_wgs84_datafile(dbfile, bounds, datakey, flatten=False):\n",
    "    \"\"\"\n",
    "    For data stored in an hdf5 data file, slice out the data set within\n",
    "    a bounding box\n",
    "\n",
    "    :param str dbfile:\n",
    "        Path to the database file\n",
    "    :param list bounds:\n",
    "        Bounding box define as [llon, llat, ulon, ulat]\n",
    "    :param str datakey:\n",
    "        Name of the dataset to be sliced\n",
    "    :param bool flatten:\n",
    "        If True then flattens the sliced data to three 1-D vectors of lon, lat\n",
    "        and data, otherwise returns the lons and lats as 1-D vectors of\n",
    "        (nlon,) and (nlat,) respectively and data as a 2-D vector (nlat, nlon)\n",
    "    \"\"\"\n",
    "    llon, llat, ulon, ulat = tuple(bounds)\n",
    "    db = h5py.File(dbfile, \"r\")\n",
    "    lons = db[\"longitude\"][:]\n",
    "    lats = db[\"latitude\"][:]\n",
    "    idx_lon = np.logical_and(lons >= llon, lons <= ulon).nonzero()[0]\n",
    "    idx_lat = np.logical_and(lats >= llat, lats <= ulat).nonzero()[0]\n",
    "    lons = lons[idx_lon]\n",
    "    lats = lats[idx_lat]\n",
    "    data = db[datakey][idx_lat, :][:, idx_lon]\n",
    "    db.close()\n",
    "    if flatten:\n",
    "        lons, lats = np.meshgrid(lons, lats)\n",
    "        return lons.flatten(), lats.flatten(), data.flatten()\n",
    "    else:\n",
    "        return lons, lats, data\n",
    "    \n",
    "def interpolate_xvf_grid(target_lons, target_lats, default_xvf=150.):\n",
    "    \"\"\"\n",
    "    Defines the volcanic front distance(xvf) at a set of locations by direct 2D\n",
    "    linear interpolation from the 5 arc minute grid\n",
    "    :param np.ndrray target_lons:\n",
    "        Vector (1D) of longitudes\n",
    "    :param np.ndarray target_lats:\n",
    "        Vector (1D) of latitudes\n",
    "    :param float default_xvf:\n",
    "        Default volcanic front distance (km)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the bounding box of the region - with a 10 arc minute buffer\n",
    "    spc = 1. / 6.\n",
    "    bbox = (np.min(target_lons) - spc, np.min(target_lats) - spc,\n",
    "            np.max(target_lons) + spc, np.max(target_lats) + spc)\n",
    "    # Extract the slice of the 5' volcanic distance grid\n",
    "    lons, lats, xvf = slice_wgs84_datafile(VF_DIST_FILE, bbox, \"xvf\")\n",
    "    if np.all(np.fabs(xvf - default_xvf) < 1.0E-3):\n",
    "        # No relevant volcanic distance within bounding box\n",
    "        return default_xvf * np.ones(target_lons.shape)\n",
    "    # Interpolate from regular grid to target sites\n",
    "    lons, lats = np.meshgrid(lons, lats)\n",
    "    ipl = RegularGridInterpolator((lons[0, :], lats[:, 0][::-1]),\n",
    "                                  xvf[::-1, :].T, bounds_error=False,\n",
    "                                  fill_value=default_xvf)\n",
    "    output_xvf = ipl(np.column_stack([target_lons.flatten(),\n",
    "                                      target_lats.flatten()]))\n",
    "    return np.reshape(output_xvf, target_lons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74787796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading locations from ../Spain/DRAFT_20110511_M5.1_Lorca/OpenQuake_gmfs/site_model_stations.csv\n",
      "Building site model for points\n",
      "---- Retrieving data within bounding box\n",
      "---- Identifying site values\n",
      "---- Getting volcanic distance\n",
      "---- Building dataframe\n",
      "Assigning the sites to the ESHM20 regions\n",
      "Exporting model to csv file ../Spain/DRAFT_20110511_M5.1_Lorca/OpenQuake_gmfs/aux_file.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "input_file=file_stations\n",
    "\n",
    "onshore_only = True\n",
    "print(\"Loading locations from %s\" % input_file)\n",
    "df = pd.read_csv(input_file, sep=\",\")\n",
    "has_lonlat = (\"lon\" in df.columns) and (\"lat\" in df.columns)\n",
    "if not has_lonlat:\n",
    "    # Check if inputs in xcoord, ycoord\n",
    "    has_crds = (\"xcoord\" in df.columns) and (\"ycoord\" in df.columns)\n",
    "    if has_crds:\n",
    "        df[\"lon\"] = df[\"xcoord\"].values\n",
    "        df[\"lat\"] = df[\"ycoord\"].values\n",
    "    else:\n",
    "        raise ValueError(\"Input file %s has no lon,lat or \"\n",
    "                         \"xcoord,ycoord attributes\" % args.input_file)\n",
    "# Dropping duplicate sites\n",
    "df.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "print(\"Building site model for points\")\n",
    "site_manager = SiteManager.site_model_from_points(df[\"lon\"].values, df[\"lat\"].values, onshore_only=onshore_only)\n",
    "\n",
    "# Save file\n",
    "\n",
    "site_manager.to_csv(aux_file)\n",
    "df = pd.read_csv(aux_file)\n",
    "df = df.assign(row_number=range(len(df)))\n",
    "df\n",
    "df['custom_site_id'] = 's_' + df['row_number'].astype(str)\n",
    "df=df.drop(columns=['row_number'])\n",
    "df.to_csv(output_file_stations,index=False)\n",
    "\n",
    "os.remove(aux_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45bcab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading locations from ../Spain/DRAFT_20110511_M5.1_Lorca/OpenQuake_gmfs/site_model.csv\n",
      "Building site model for points\n",
      "---- Retrieving data within bounding box\n",
      "---- Identifying site values\n",
      "---- Getting volcanic distance\n",
      "---- Building dataframe\n",
      "Assigning the sites to the ESHM20 regions\n",
      "Exporting model to csv file ../Spain/DRAFT_20110511_M5.1_Lorca/OpenQuake_gmfs/aux_file.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "/Users/chago/openquake/lib/python3.9/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "input_file=file_site_model\n",
    "\n",
    "onshore_only = True\n",
    "print(\"Loading locations from %s\" % input_file)\n",
    "df = pd.read_csv(input_file, sep=\",\")\n",
    "has_lonlat = (\"lon\" in df.columns) and (\"lat\" in df.columns)\n",
    "if not has_lonlat:\n",
    "    # Check if inputs in xcoord, ycoord\n",
    "    has_crds = (\"xcoord\" in df.columns) and (\"ycoord\" in df.columns)\n",
    "    if has_crds:\n",
    "        df[\"lon\"] = df[\"xcoord\"].values\n",
    "        df[\"lat\"] = df[\"ycoord\"].values\n",
    "    else:\n",
    "        raise ValueError(\"Input file %s has no lon,lat or \"\n",
    "                         \"xcoord,ycoord attributes\" % args.input_file)\n",
    "# Dropping duplicate sites\n",
    "df.drop_duplicates([\"lon\", \"lat\"], inplace=True)\n",
    "print(\"Building site model for points\")\n",
    "site_manager = SiteManager.site_model_from_points(df[\"lon\"].values, df[\"lat\"].values, onshore_only=onshore_only)\n",
    "\n",
    "# Save file\n",
    "\n",
    "site_manager.to_csv(aux_file)\n",
    "df = pd.read_csv(aux_file)\n",
    "df = df.assign(row_number=range(len(df)))\n",
    "df\n",
    "df['custom_site_id'] = 't_' + df['row_number'].astype(str)\n",
    "df=df.drop(columns=['row_number'])\n",
    "df.to_csv(output_file_site_model,index=False)\n",
    "\n",
    "os.remove(aux_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7113891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
