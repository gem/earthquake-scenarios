{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e803765",
   "metadata": {},
   "source": [
    "# Create `stationlist.csv` and `site_model_stations.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65150492",
   "metadata": {},
   "source": [
    "##  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0962ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import usgs_get_vs30\n",
    "\n",
    "from openquake.commands.prepare_site_model import main as oq_site_model\n",
    "from openquake.hazardlib.site import calculate_z1pt0, calculate_z2pt5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b706a100",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events list from ECD\n",
    "event = 'DRAFT_20150425_M7.8_Gorkha'\n",
    "\n",
    "try:\n",
    "    file_path = glob.glob(os.path.join(\n",
    "        '..', '*', event, \n",
    "        'Recording_Stations', \n",
    "        'Stations_Unique.csv'))[0]\n",
    "except IndexError:\n",
    "    # If not `Stations_Unique` then use the only Stations_REF available\n",
    "    stsfiles = glob.glob(os.path.join(\n",
    "        '..', '*', event, \n",
    "        'Recording_Stations', \n",
    "        'Stations_*.csv'))\n",
    "    assert len(stsfiles) == 1, 'Multiple Stations_*.csv files found but no Unique file'\n",
    "    file_path = stsfiles[0]\n",
    "\n",
    "print('Reference file for OQ calculation:\\n   ', \n",
    "      file_path)\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "220aa33d-9d4c-420d-9e81-eaad8dc449e5",
   "metadata": {},
   "source": [
    "## Prepare `stationlist.csv` following OQ format\n",
    "- Remove columns with missing IM values (OQ only accepts stations with values in all IMs)\n",
    "- Remove Vs30 data (it will be provided in the site model file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b851dbe-b0cc-4f0f-bd58-8e2fd8a31b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with missing IM values\n",
    "\n",
    "seismic = df.STATION_TYPE == 'seismic'\n",
    "macroseismic = df.STATION_TYPE == 'macroseismic'\n",
    "print('Number of stations:', len(df))\n",
    "print('  # seismic stations:', len(df[seismic]))\n",
    "print('  # macroseicmic stations:', len(df[macroseismic]))\n",
    "\n",
    "# Find columns with values\n",
    "vals = [x for x in df.columns if x.endswith('_VALUE')]\n",
    "\n",
    "# Print the number of missing values in columns\n",
    "print('\\nMissing vals in SEISMIC stations:\\n', \n",
    "      df.loc[seismic, vals].isnull().sum())\n",
    "\n",
    "print('\\nMissing vals in MACROSEISMIC stations:\\n', \n",
    "      df.loc[macroseismic, vals].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569555d6-82ba-4d5c-860b-65c471d3ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select IMTs to use in OQ\n",
    "imts = ['PGA_VALUE'] # Other of interest: 'SA(0.3)_VALUE', 'SA(0.6)_VALUE', 'SA(1.0)_VALUE'\n",
    "\n",
    "# Columns to be used in OQ \n",
    "cols = ['STATION_ID', 'STATION_NAME', 'LONGITUDE', 'LATITUDE', 'STATION_TYPE']\n",
    "\n",
    "for imt in imts:\n",
    "    cols.append(imt)\n",
    "    cols.append(imt.replace('VALUE', 'LN_SIGMA'))\n",
    "\n",
    "cols_vs = []\n",
    "for vcol in ['SOIL_TYPE', 'VS30', 'VS30_TYPE']:\n",
    "    if vcol in df.columns:\n",
    "        cols_vs.append(vcol)\n",
    "        \n",
    "# Drop rows with empty values\n",
    "df = df[cols + cols_vs].dropna(axis=0, how='any', subset=imts)\n",
    "\n",
    "# OQ only reads 5 decimal places\n",
    "df = df.round(5).copy()\n",
    "\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(subset=['LONGITUDE', 'LATITUDE'], inplace=True)\n",
    "\n",
    "print('Filtered number of stations:', len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "045b7b53-8917-457f-a1e6-7cd0d5684276",
   "metadata": {},
   "source": [
    "### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48aa0b4-1679-47f8-b893-7983381aacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files in folder OpenQuake_gmfs\n",
    "folder = file_path[:file_path.find('Recording_Stations')]\n",
    "stations = df[cols].copy()\n",
    "\n",
    "seismic = stations.STATION_TYPE == 'seismic'\n",
    "\n",
    "if 'seismic' in stations.STATION_TYPE.unique():\n",
    "    # Save stationlis with only seismic stations\n",
    "    out_path = os.path.join(folder, 'OpenQuake_gmfs', 'stationlist_seismic.csv')\n",
    "    stations[seismic].to_csv(out_path, encoding='utf-8', index=False)\n",
    "    print('\\n Saving:\\n', out_path)\n",
    "\n",
    "if 'macroseismic' in stations.STATION_TYPE.unique():\n",
    "    # Save stationlis complete file (seismic + macroseismic)\n",
    "    out_path = os.path.join(folder, 'OpenQuake_gmfs', 'stationlist_all.csv')\n",
    "    stations.to_csv(out_path, encoding='utf-8', index=False)\n",
    "    print('\\n Saving:\\n', out_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2177d830-2d74-438a-acbd-9e14898681b7",
   "metadata": {},
   "source": [
    "## Prepare `site_model_stations.csv` file\n",
    "- Add qualitative Vs30 values\n",
    "- Add missing Vs30 from USGS\n",
    "- Estimate z1 and z2.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0830246-7f74-49d0-aab8-afc3b45affe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adjust Vs30 qualitative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec7b53-3a1e-4b18-8426-84135234e5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame for site model\n",
    "df_soil = df[['STATION_TYPE', 'LONGITUDE', 'LATITUDE'] + cols_vs].copy()\n",
    "\n",
    "# Check qualitative soit types\n",
    "if 'SOIL_TYPE' in df_soil.columns:\n",
    "    print('SOIL_TYPE options:\\n', df_soil.SOIL_TYPE.unique())\n",
    "\n",
    "    # Define qualitative descriptions for rock or very stiff soil\n",
    "    rock = ['ROCA', 'Roca', 'roca', 'ROCK', 'Rock', 'rock', \n",
    "            'ROCA BASALTICA', 'ROCA SEDIMENTARIA', 'ROCA  CANTERA']\n",
    "    \n",
    "    station_at_rock = df_soil.SOIL_TYPE.isin(rock)\n",
    "    \n",
    "    # Assign value for rock soil\n",
    "    print(f'\\nFound {station_at_rock.sum()} stations with rock conditions')\n",
    "    df_soil.loc[station_at_rock, 'VS30'] = 800\n",
    "    df_soil.loc[station_at_rock, 'VS30_TYPE'] = 'Inferred'\n",
    "    print('\\nAssociating rock values to Vs30 = 800 m/s')\n",
    "\n",
    "# Print message if there are columns with no Vs30 values\n",
    "if df_soil.VS30.isnull().any():\n",
    "    miss_vals = df_soil.VS30.isnull().sum()\n",
    "    msg = f'{miss_vals} values missing Vs30 reference data'\n",
    "    print(f'\\x1b[0;31m \\n{msg} \\x1b[0m')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1791fd6e-a560-4521-831d-25bc30ce2bd8",
   "metadata": {},
   "source": [
    "### Add missing Vs30 values from USGS reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25169499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add VS30 data from USGS values\n",
    "\n",
    "# Vs30 reference file (no headers, use USGS format)\n",
    "# When running from Wilson, it's possible to use the worldv30 dataset\n",
    "# located at '/home/risk/sites_vs30/original/vs30mosaic.hdf5'\n",
    "vs30_path = '../vs30mosaic.hdf5'\n",
    "\n",
    "# Use `oq prepare_site_model` to get values\n",
    "df_soil = usgs_get_vs30.add_vs30_from_ref(df_soil, [vs30_path])\n",
    "\n",
    "# Add z1pt0 and z2pt5\n",
    "df_soil['z1pt0'] = calculate_z1pt0(df_soil.VS30)\n",
    "df_soil['z2pt5'] = calculate_z2pt5(df_soil.VS30)\n",
    "df_soil['vs30measured'] = 0\n",
    "\n",
    "# Print values\n",
    "df_soil.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "754f38d4-fc85-4227-9667-0be616dad6d8",
   "metadata": {},
   "source": [
    "### Generate `site_model_stations.csv` in OQ format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c2a5b-c447-4738-a675-3fc6a1c4d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "oq_sites = df_soil.copy()\n",
    "\n",
    "# Select columns and name them for OQ\n",
    "oq_sites.rename(columns={'LONGITUDE': 'lon', \n",
    "                        'LATITUDE': 'lat',\n",
    "                        'VS30': 'vs30',\n",
    "                        }, inplace=True)\n",
    "\n",
    "if len(df_soil) != len(oq_sites):\n",
    "    print(f'Drop {len(df_soil) - len(oq_sites)} duplicates based on priority')\n",
    "\n",
    "# Rename site_id\n",
    "site_id = [str(site)[0] + '_' + str(n) for n, site in \n",
    "           enumerate(oq_sites.loc[:, 'STATION_TYPE'])]\n",
    "oq_sites.loc[:, 'custom_site_id'] = site_id\n",
    "\n",
    "# Save site_model_stations file\n",
    "cols = ['custom_site_id', 'lon', 'lat', 'vs30','vs30measured','z1pt0','z2pt5']\n",
    "out_path = os.path.join(folder, 'OpenQuake_gmfs', 'site_model_stations.csv')\n",
    "oq_sites[cols].to_csv(out_path, encoding='utf-8', index=False)\n",
    "print('\\n Saving:\\n', out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
